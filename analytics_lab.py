# -*- coding: utf-8 -*-
"""analytics_lab.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/195JbglJ4094LKhlNooJDI_Lu3cmRp7q7
"""

pip install pyspark

#@title Default title text
## Download Input dataset for analytics
# https://propensity-labs-screening.s3.amazonaws.com/analytics/analytics_input.csv

#Create Spark Session
from pyspark.sql import SparkSession
spark = SparkSession\
        .builder \
        .appName("Propensitylabs") \
        .getOrCreate()

#Read CSV to Dataframe
data = spark.read\
            .format("csv")\
            .option("header","true")\
            .load("analytics_input.csv")
data.cache()

#Display Scheme
data.printSchema()

#Change Column names to remove spaces, lower case and replace spaces in column names with "_" (underscore)
from pyspark.sql.functions import col, ltrim
for i in data.columns:
  n = i.lstrip()
  n = n.rstrip()
  n = n.lower()
  n = n.replace(" ","_")
  data = data.withColumnRenamed(i, n)

data.printSchema()

#Identify all rows with Null values for provider id
from pyspark.sql.functions import col

null_provider_id_rows = data.filter(col("provider_id").isNull())
null_provider_id_rows.show()

#Change fields with Payment information into numbers by using Lambda functions in python
from pyspark.sql.functions import udf
from pyspark.sql.types import FloatType
convert_to_number = udf(lambda payment_str: float(payment_str.replace('$', '')), FloatType())

payment_columns = ['average_covered_charges','average_total_payments', 'average_medicare_payments']
for col_name in payment_columns:
  data = data.withColumn(col_name, convert_to_number(col_name))
data.show()

#Plot a Bar chart by state and "Average Total Payments" and zip code vs "total_charges"
state_vs_Average_total_payment_df = data.groupBy('provider_state').avg('average_total_payments')
state_vs_Average_total_payment_pd_df = state_vs_Average_total_payment_df.toPandas()
state_vs_Average_total_payment_pd_df.plot.bar(x="provider_state", y="avg(average_total_payments)")

data = data.withColumn("total_discharges", col("total_discharges").cast("int"))

zip_code_vs_total_discharge_df = data.groupBy('provider_zip_code').sum('total_discharges')
zip_code_vs_total_discharge_pd_df = zip_code_vs_total_discharge_df.toPandas()

zip_code_vs_total_discharge_pd_df.plot.bar(x="provider_zip_code", y="sum(total_discharges)")

#BONUS - Plot intersting statistics from the dataset
provider_state_with_total_discharge_df = data.groupBy('provider_state').sum('total_discharges')
provider_state_vs_total_discharge_pd_df = provider_state_with_total_discharge_df.toPandas()
provider_state_vs_total_discharge_pd_df.plot.bar(x="provider_state", y="sum(total_discharges)")

state_vs_average_medicare_payments_df = data.groupBy('provider_state').avg('average_medicare_payments')
state_vs_average_medicare_payment_pd_df = state_vs_average_medicare_payments_df.toPandas()
state_vs_average_medicare_payment_pd_df.plot.bar(x="provider_state", y="avg(average_medicare_payments)")

state_vs_average_covered_charges_df = data.groupBy('provider_state').avg('average_covered_charges')
state_vs_average_covered_charges_pd_df = state_vs_average_covered_charges_df.toPandas()
state_vs_average_covered_charges_pd_df.plot.bar(x="provider_state", y="avg(average_covered_charges)")

from pyspark.sql.functions import avg, max, min
state_with_min_and_max_total_discharge_df = data.groupBy("provider_state").agg(min("total_discharges"), max("total_discharges"))
state_with_min_and_max_total_discharge_df.show()

city_with_min_and_max_total_discharge_df = data.groupBy("provider_city").agg(min("total_discharges"), max("total_discharges"))
city_with_min_and_max_total_discharge_df.show()

#Write the Dataset to a Parquet format partitioned by State
data.write.partitionBy('provider_state').parquet("Parquet")